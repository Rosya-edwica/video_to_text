import os
import logging
from time import perf_counter
import argparse
import json
from datetime import datetime
import config

import audio
import video
import gpt
from kafka_test import NewProducer

VIDEOS_DIR = "data/videos/"
GPT_QUERY = "Ты профессионал в обработке большого текста. Мне нужно проверить своих студентов на знание текста.\nСоставь тест из 10 вопросов. Важно, чтобы вопросы были сложными и не содержали в себе подсказок. Ответы должны отличаться друг от друга (нельзя допустить, чтобы ответ всегда был в пункте a) например. Структура твоего ответа: '1. Вопрос\na) первый вариант\nb) второй вариант\nc) третий вариант\nd) четвертый вариант\nОтвет: полный вариант'. Текст из которого нужно брать ответы на эти вопросы:\n"


arg_parser = argparse.ArgumentParser()
arg_parser.add_argument("-m", "--method", required=True, help="""
    Выберите метод для транскрибации видео:,
    1. whisper - транскрибация с помощью openai,
    2. google - транскрибация с помощью google_recognize (бесплатный, но не качественный результат)""")
args = arg_parser.parse_args()

logging.basicConfig(filename="logs.log", filemode='a', format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',
                    datefmt='%H:%M:%S', level=logging.DEBUG)

if args.method not in ("whisper", "google"):
    exit("Неправильный аргумент")

def main():
    print("Selected method:", args.method)
    os.makedirs(VIDEOS_DIR, exist_ok=True)
    if len(os.listdir(VIDEOS_DIR)) == 0:
        exit(f"Добавьте хотя бы одно видео формата .mp4 в папку: {VIDEOS_DIR}")

    producer = NewProducer()
    transcriber = audio.Transcriber()
    for file in os.listdir(VIDEOS_DIR):
        start_time = perf_counter()
        video_file = os.path.join(VIDEOS_DIR, file)
        audio_file = video.video_to_audio(video_file, audio_format="mp3")

        transcriber.set_new_audio_path(audio_file)
        text = transcriber.transcribe(min_per_split=1, method=args.method)
        answer = gpt.create_test_by_text(GPT_QUERY, text)
        if answer:
            logging.info(msg=f"Время на один файл: {perf_counter() - start_time}\tСтоимость запроса:{answer.Cost.Dollar}\tКоличество токенов:{answer.AnswerTokens + answer.QuestionTokens}")
            logging.info(msg=f"Тест по тексту: {answer.Text}")
            gpt.save_answer(answer.Text, file.replace(".mp4", ".txt"))
            test = config.parse_test(answer.Text)
            config.save_test_to_json(test, file.replace(".mp4", ".json"))
            kafka_message = {
                "date": str(datetime.now()),
                "promt": GPT_QUERY + text,
                "answer": answer.Text,
                "cost_usd": answer.Cost.Dollar,
                "tokens": answer.AnswerTokens + answer.QuestionTokens,
                "time_exe": perf_counter() - start_time
            }
            producer.send_to_text_partition(json.dumps(kafka_message))
        

if __name__ == "__main__":
    main()
    # answer = gpt.create_test_by_text(query="Составь техническое задание из следующего текста:", text='Обработка навыков\n\n### Подбор навыков для курсов\nДля образовательных программ, предлагаемых на платформе Эдвики, которые не имеют прямых связей с конкретными навыками, был использован следующий запрос к GPT\n```\nКакие навыки я получу после прохождения данного курса "%s"? Составь список из 20 навыков. Навыки должны быть в виде ключевых слов.\n```\nНавыки, в этом контексте, должны быть представлены в форме ключевых слов. После того как GPT успешно сформировала список навыков, которые могут быть приобретены в результате прохождения конкретного курса, был проведен поиск этих навыков в нашей существующей базе данных.\n\nЕсли навык, предложенный GPT, уже присутствовал в нашей базе данных, мы устанавливали связь между этим навыком и соответствующим курсом. Однако, если навыка не было в базе, мы пропускали его. Это было сделано с целью предотвращения необоснованного расширения таблицы навыков, что могло бы привести к ее избыточности и усложнению работы с ней.\nПосле того как GPT составил список навыков, относящихся к конкретному курсу, проводился поиск этих навыков в нашей базе. Если навык, предложенный GPT, существовал в нашей базе, то мы проставляли связь между этим навыком и курсом. Если навыка не было в базе, то мы его пропускали. Это было сделано с целью остановить расширения таблицы навыков. \n\n\n### Подбор навыков для профессий\nДля определения набора навыков, необходимых для профессий Эдики, которые не имеют прямых связей с конкретными навыками, использовался следующий запрос к GPT\n```\nСоставь список из 20 навыков и знаний для профессии  "%s". \n\tПиши коротко, не более четырех слов. Не указывай банальные знания и навыки, пиши только Hard Skills и не указывай Soft Skills. \n\tНе используй в описании такие слова как: знание, умение, владение, работа с, понимание, использование. \n\tНавыки должны относиться к профобласти "%s"`\n```\nДля успешного выполнения этого запроса необходимо предоставить название профессии и область профессиональной деятельности, к которой она относится.\n\nЕсли навык, предложенный GPT, уже присутствовал в нашей базе данных, мы устанавливали связь между этим навыком и соответствующей профессией. В случае, если навыка не было в нашей базе, мы сохраняли его как новый элемент и также устанавливали связь между этим новым навыком и профессией.\n\n\nДля выполнения этого запроса требуется название профессии и профобласть, к которой она принадлежит. \n\nЕсли навык, предложенный GPT, существовал в нашей базе, то мы проставляли связь между этим навыком и профессией. Если навыка не было в базе, то мы его сохраняли как новый и тоже проставляли связь между профессией и навыком. \n\n\n### Обработка от мусора навыков\nПроцесс обработки данных можно разделить на следующие этапы:\n\n1. Трансляция навыков: некоторые из навыков были изначально получены из вакансий, включая вакансии, опубликованные на английском языке. В связи с этим, нам требуется выполнить перевод, чтобы привести навык, обозначенный как "responsibility", к его русскоязычному эквиваленту - "ответственность". Однако стоит отметить, что некоторые навыки, такие как Python или Excel, останутся неизменными.\n\n2. Очистка от избыточных символов: символы, такие как «*?|•#«»’/\\», полностью удаляются из текста. В то же время символы «_-» заменяются на пробелы.\n\n3. Идентификация и удаление стоп-слов: мы ищем и удаляем навыки, которые содержат стоп-слова. Список стоп-слов весьма обширен и включает в себя банальные навыки (например, коммуникабельность, ответственность и т.д.), названия городов, профессий и другие слова.\n\n4. Приведение навыков к их начальной форме: например, фраза "владение английским языком" будет преобразована в "владение английский язык". Это необходимо для того, чтобы искать дубликаты среди навыков, независимо от их рода, падежа или числа.\n\n5. Поиск дубликатов по написанию: мы ищем навыки, которые полностью совпадают по написанию. В случае обнаружения таких дубликатов, один из них будет удален.\n\n6. Поиск дубликатов по нечеткому совпадению: это процесс поиска навыков, которые не полностью, но в значительной степени совпадают по написанию.\n\n7. Идентификация стоп-слов с использованием регулярных выражений: это процесс поиска и удаления стоп-слов с помощью специальных шаблонов, называемых регулярными выражениями.\n\n\nСам алгоритм очистки можно представить следующим образом:\n1. Загружается список навыков из БД или файла. \n2.  Осуществляется перевод навыков - это позволяет увеличить вероятность успешного поиска дубликатов. Этот процесс реализуется с использованием специализированной библиотеки для перевода слов, которая базируется на функционале популярного сервиса Google Translate.  Это позволяет обеспечить высокую точность перевода и эффективность поиска дубликатов, учитывая различные языковые особенности и нюансы\n3.  Осуществляется удаление всех избыточных и ненужных знаков препинания, которые могут быть обнаружены в тексте. Удаление лишних знаков препинания помогает улучшить качество поиска, уменьшая количество ложных совпадений и повышая общую эффективность работы системы.\n4.  Применяем метод приведения слов к их инфинитивной форме. Это процесс, который включает в себя преобразование слов в их базовую, неизмененную форму, что позволяет нам упростить и унифицировать процесс поиска и сопоставления данных. Приведение слов к инфинитиву играет ключевую роль в нашей системе обработки данных, поскольку это позволяет нам искать дубликаты навыков независимо от рода, числа и падежа слов, что значительно увеличивает эффективность нашего проекта. Это также упрощает процесс анализа и сопоставления данных, поскольку все слова приводятся к единой, стандартной форме. В качестве реализации выбрана библиотека Python PyMorphy2\n5.  Проводим процедуру удаления так называемых стоп слов, которые были заранее определены нами как ненужные для функционирования нашей системы. Это могут быть слова, которые не несут в себе значимой информации для анализа или обработки данных, и их присутствие может только затруднить работу системы. К примеру, в качестве стоп слов могут быть использованы названия городов или профессий. Это означает, что при обработке данных, все упоминания этих слов будут игнорироваться системой, так как они не несут в себе необходимой информации для выполнения поставленных задач.\n6.  Теперь начинаем сравнивать навыки друг с другом, чтобы найти навыки похожие по написанию на 100%. Для этого мы проводим тщательное сравнение каждого навыка с другими, используя специализированные алгоритмы и методы сравнения. Это позволяет нам обнаружить дубликаты, которые могут быть скрыты из-за различий в орфографии, синтаксисе или стилистике написания. \n7.  Одним из ключевых этапов нашей работы является сравнение навыков между собой, для чего мы применяем такие инструменты, как Spacy и FuzzyWuzzy.\n\tSpacy - это библиотека для обработки естественного языка, которая предоставляет функционал для токенизации, лемматизации, распознавания именованных сущностей и других задач NLP. Она позволяет нам анализировать и сравнивать навыки на более глубоком уровне, учитывая контекст и семантику.\n\n\tFuzzyWuzzy, с другой стороны, это библиотека Python, которая используется для сравнения строк. Она использует алгоритм Levenshtein Distance для измерения различий между последовательностями символов, что позволяет нам сравнивать навыки на основе их текстового представления.\n\n')
    # f = open("tech.txt", mode="w", encoding="utf-8")
    # f.write(answer.Text)
    # f.close()